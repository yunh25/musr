import { WebSocket } from "ws";
import { AppI, TranscriptI } from "./models";
import { DisplayRequest } from "./layouts";
import { Transform } from "stream";
import { ConversationTranscriber, PushAudioInputStream } from "microsoft-cognitiveservices-speech-sdk";
import { ExtendedStreamType } from "./streams";
import pino from "pino";
/**
 * Session for an application
 */
/**
 * Audio processor configuration
 */
export interface AudioProcessorConfig {
    threshold: number;
    ratio: number;
    attack: number;
    release: number;
    gainDb: number;
    sampleRate: number;
    channels: number;
}
/**
 * Audio processor interface
 */
export interface AudioProcessorI extends Transform {
}
/**
 * The display manager interface
 */
export interface DisplayManagerI {
    handleDisplayEvent(displayRequest: DisplayRequest, userSession: UserSession): boolean;
    handleAppStart(packageName: string, userSession: UserSession): void;
    handleAppStop(packageName: string, userSession: UserSession): void;
}
/**
 * Currently active display
 */
export interface ActiveDisplay {
    displayRequest: DisplayRequest;
    startedAt: Date;
    expiresAt?: Date;
}
/**
 * User session with glasses client
 */
export interface UserSession {
    sessionId: string;
    userId: string;
    startTime: Date;
    disconnectedAt: Date | null;
    logger: pino.Logger;
    installedApps: AppI[];
    activeAppSessions: string[];
    loadingApps: Set<string>;
    appSubscriptions: Map<string, ExtendedStreamType[]> | object;
    appConnections: Map<string, WebSocket>;
    websocket: WebSocket;
    transcript: TranscriptI;
    pushStream?: PushAudioInputStream;
    recognizer?: ConversationTranscriber;
    isTranscribing: boolean;
    lastAudioTimestamp?: number;
    isGracefullyClosing?: boolean;
    bufferedAudio: ArrayBufferLike[];
    audioProcessor?: AudioProcessorI;
    isAudioProcessing?: boolean;
    whatToStream: ExtendedStreamType[];
}
/**
 * App session within a user session
 */
//# sourceMappingURL=user-session.d.ts.map